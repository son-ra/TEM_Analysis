{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49a335da-1ccc-488a-8d94-86bc82a15326",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "#import optshrink as opt # package we create\n",
    "import numpy as np\n",
    "# import scipy.io as sio\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import cartopy.crs as ccrs\n",
    "import seaborn as sns\n",
    "from shapely.geometry import Point\n",
    "import geopandas as gp\n",
    "from geodatasets import get_path\n",
    "from shapely.geometry import Polygon\n",
    "import argparse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b5e9d51-b8f1-454c-af53-846a81c957af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_col_names = ['lon'\n",
    ",'lat'\n",
    ",'variable'\n",
    ",'cohort_number'\n",
    ",'stand_age'\n",
    ",'potential_veg'\n",
    ",'current_veg'\n",
    ",'community_type'\n",
    ",'subtype'\n",
    ",'silt_clay'  ###check this order\n",
    ",'lc_state'\n",
    ",'land_area'\n",
    ",'cohort_area'\n",
    ",'year'\n",
    ",'annual_sum'\n",
    ",'monthly_maximum'\n",
    ",'monthly_mean'\n",
    ",'monthly_minimum'\n",
    ",'Jan'\n",
    ",'Feb'\n",
    ",'Mar'\n",
    ",'Apr'\n",
    ",'May'\n",
    ",'Jun'\n",
    ",'Jul'\n",
    ",'Aug'\n",
    ",'Sep'\n",
    ",'Oct'\n",
    ",'Nov'\n",
    ",'Dec'\n",
    ",'region']\n",
    "\n",
    "\n",
    "\n",
    "forest_vegs = [4, 5, 6, 8, 9, 10, 11, 16, 17, 18, 19, 20, 25, 33]\n",
    "forest_types = [\"Boreal Forest\", \"Forested Boreal Wetlands\", \"Boreal Woodlands\",\"Mixed Temperate Forests\", \n",
    "               \"Temperate Coniferous Forests\", \"Temperate Deciduous Forests\", \"Temperate Forested Wetlands\", \n",
    "               \"Tropical Evergreen Forests\", \"Tropical Forested Wetlands\", \"Tropical Deciduous Forests\", \"Xeromorphic Forests and Woodlands\"\n",
    "               ,\"Tropical Forested Floodplains\", \"Temperate Forested Floodplains\", \"Temperate Broadleaved Evergreen Forests\"]\n",
    "\n",
    "forest_pfts = pd.DataFrame({\n",
    "'current_veg':forest_vegs, \n",
    "    'forest_type':forest_types\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50686835-2e0a-4fd8-ad18-66570685a6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ensemble_dir = '/group/moniergrp/TEM_Large_Ensemble/run_support_files/cru_regions/'\n",
    "\n",
    "output_dir_path = '/group/moniergrp/TEM_Large_Ensemble/output_files/cru_historical/' \n",
    "intervals_standage = np.concatenate((np.arange(-1, 100, 5),np.array([124, 149, 3000])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0564403b-c09f-49fe-8c8a-4d43ebf1dbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = 'region_14'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a678ac6d-1350-4ec2-b188-9771e13b43eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2088733/233635757.py:7: DtypeWarning: Columns (14,15,16,20,21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(ensemble_dir+region+'/var_out/'+file, names = output_col_names)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "all_region = pd.DataFrame()\n",
    "all_region_stand_age = pd.DataFrame()\n",
    "\n",
    "# for region in regions:\n",
    "files = os.listdir(ensemble_dir+region+'/var_out/')\n",
    "for file in files:\n",
    "    data = pd.read_csv(ensemble_dir+region+'/var_out/'+file, names = output_col_names)\n",
    "    if len(data) > 0:\n",
    "        # print(len(data))\n",
    "        data=data.merge(forest_pfts, on = 'current_veg', how = 'inner')\n",
    "        # print(len(data))\n",
    "        data['stand_age_bin'] = pd.cut(\n",
    "                    data['stand_age'], bins=intervals_standage)\n",
    "        data['stand_age_interval_min'] = data['stand_age_bin'].apply(lambda x: x.left).astype(int) + 1\n",
    "        data['file'] = file\n",
    "        data['monthly_mean'] = data['monthly_mean'].astype(float)\n",
    "        data['Jan'] = data['Jan'].astype(float)\n",
    "        data['Feb'] = data['Feb'].astype(float)\n",
    "        data['Mar'] = data['Mar'].astype(float)\n",
    "        data['Apr'] = data['Apr'].astype(float)\n",
    "        data['May'] = data['May'].astype(float)\n",
    "        data['Jun'] = data['Jun'].astype(float)\n",
    "        data['Jul'] = data['Jul'].astype(float)\n",
    "        data['Aug'] = data['Aug'].astype(float)\n",
    "        data['Sep'] = data['Sep'].astype(float)\n",
    "        data['Oct'] = data['Oct'].astype(float)\n",
    "        data['Nov'] = data['Nov'].astype(float)\n",
    "        data['Dec'] = data['Dec'].astype(float)\n",
    "\n",
    "        #####create weight for variables\n",
    "        data['monthly_mean_weight'] = data['monthly_mean']* data['cohort_area']\n",
    "        data['Jan_weight'] = data['Jan']* data['cohort_area']\n",
    "        data['Feb_weight'] = data['Feb']* data['cohort_area']\n",
    "        data['Mar_weight'] = data['Mar']* data['cohort_area']\n",
    "        data['Apr_weight'] = data['Apr']* data['cohort_area']\n",
    "        data['May_weight'] = data['May']* data['cohort_area']\n",
    "        data['Jun_weight'] = data['Jun']* data['cohort_area']\n",
    "        data['Jul_weight'] = data['Jul']* data['cohort_area']\n",
    "        data['Aug_weight'] = data['Aug']* data['cohort_area']\n",
    "        data['Sep_weight'] = data['Sep']* data['cohort_area']\n",
    "        data['Oct_weight'] = data['Oct']* data['cohort_area']\n",
    "        data['Nov_weight'] = data['Nov']* data['cohort_area']\n",
    "        data['Dec_weight'] = data['Dec']* data['cohort_area']\n",
    "\n",
    "\n",
    "        #####group by stand age bin\n",
    "        data = data.groupby(\n",
    "            ['lon','lat','year','variable','stand_age_interval_min','forest_type','current_veg','community_type','silt_clay','region','file']\n",
    "        )[data.columns[data.columns.str.contains('weight|area')]\n",
    "        ].sum()\n",
    "        # ####recalculate values\n",
    "        data=data.reset_index()\n",
    "        # ### finish weighted average calculation\n",
    "        data['monthly_mean'] = (data['monthly_mean_weight']/ data['cohort_area'])\n",
    "        data['Jan'] = (data['Jan_weight']/ data['cohort_area'])\n",
    "        data['Feb'] = (data['Feb_weight']/ data['cohort_area'])\n",
    "        data['Mar'] = (data['Mar_weight']/ data['cohort_area'])\n",
    "        data['Apr'] = (data['Apr_weight']/ data['cohort_area'])\n",
    "        data['May'] = (data['May_weight']/ data['cohort_area'])\n",
    "        data['Jun'] = (data['Jun_weight']/ data['cohort_area'])\n",
    "        data['Jul'] = (data['Jul_weight']/ data['cohort_area'])\n",
    "        data['Aug'] = (data['Aug_weight']/ data['cohort_area'])\n",
    "        data['Sep'] = (data['Sep_weight']/ data['cohort_area'])\n",
    "        data['Oct'] = (data['Oct_weight']/ data['cohort_area'])\n",
    "        data['Nov'] = (data['Nov_weight']/ data['cohort_area'])\n",
    "        data['Dec'] = (data['Dec_weight']/ data['cohort_area'])\n",
    "\n",
    "        # ###give relevant information\n",
    "\n",
    "        all_region_stand_age = pd.concat([all_region_stand_age, data])\n",
    "\n",
    "        ####group by lat/lon/year\n",
    "        data = data.groupby(\n",
    "            ['lon','lat','year','variable','forest_type','current_veg','community_type','silt_clay','region','file']\n",
    "        )[data.columns[data.columns.str.contains('weight|area')]\n",
    "        ].sum()\n",
    "        # print(len(data))\n",
    "        # ###sum up by lat lon year var\n",
    "\n",
    "        # ####recalculate values\n",
    "        data=data.reset_index()\n",
    "        # ### finish weighted average calculation\n",
    "        data['monthly_mean'] = (data['monthly_mean_weight']/ data['cohort_area'])\n",
    "        data['Jan'] = (data['Jan_weight']/ data['cohort_area'])\n",
    "        data['Feb'] = (data['Feb_weight']/ data['cohort_area'])\n",
    "        data['Mar'] = (data['Mar_weight']/ data['cohort_area'])\n",
    "        data['Apr'] = (data['Apr_weight']/ data['cohort_area'])\n",
    "        data['May'] = (data['May_weight']/ data['cohort_area'])\n",
    "        data['Jun'] = (data['Jun_weight']/ data['cohort_area'])\n",
    "        data['Jul'] = (data['Jul_weight']/ data['cohort_area'])\n",
    "        data['Aug'] = (data['Aug_weight']/ data['cohort_area'])\n",
    "        data['Sep'] = (data['Sep_weight']/ data['cohort_area'])\n",
    "        data['Oct'] = (data['Oct_weight']/ data['cohort_area'])\n",
    "        data['Nov'] = (data['Nov_weight']/ data['cohort_area'])\n",
    "        data['Dec'] = (data['Dec_weight']/ data['cohort_area'])\n",
    "\n",
    "        # ###give relevant information\n",
    "\n",
    "        all_region = pd.concat([all_region, data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0c0479c-3f55-4b3b-a442-11227fd8362e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3593903/457289002.py:1: DtypeWarning: Columns (14,15,16,18,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(ensemble_dir+region+'/var_out/'+file, names = output_col_names)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(ensemble_dir+region+'/var_out/'+file, names = output_col_names)\n",
    "data['monthly_mean'] = data['monthly_mean'].astype(float)\n",
    "data['Jan'] = data['Jan'].astype(float)\n",
    "data['Feb'] = data['Feb'].astype(float)\n",
    "data['Mar'] = data['Mar'].astype(float)\n",
    "data['Apr'] = data['Apr'].astype(float)\n",
    "data['May'] = data['May'].astype(float)\n",
    "data['Jun'] = data['Jun'].astype(float)\n",
    "data['Jul'] = data['Jul'].astype(float)\n",
    "data['Aug'] = data['Aug'].astype(float)\n",
    "data['Sep'] = data['Sep'].astype(float)\n",
    "data['Oct'] = data['Oct'].astype(float)\n",
    "data['Nov'] = data['Nov'].astype(float)\n",
    "data['Dec'] = data['Dec'].astype(float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0578a765-078b-42fb-a6db-5380a473729d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>variable</th>\n",
       "      <th>cohort_number</th>\n",
       "      <th>stand_age</th>\n",
       "      <th>potential_veg</th>\n",
       "      <th>current_veg</th>\n",
       "      <th>community_type</th>\n",
       "      <th>subtype</th>\n",
       "      <th>silt_clay</th>\n",
       "      <th>...</th>\n",
       "      <th>Apr</th>\n",
       "      <th>May</th>\n",
       "      <th>Jun</th>\n",
       "      <th>Jul</th>\n",
       "      <th>Aug</th>\n",
       "      <th>Sep</th>\n",
       "      <th>Oct</th>\n",
       "      <th>Nov</th>\n",
       "      <th>Dec</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>114.5</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NETNMIN</td>\n",
       "      <td>1</td>\n",
       "      <td>3000</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>59.64</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.73</td>\n",
       "      <td>-8.65</td>\n",
       "      <td>0.62</td>\n",
       "      <td>-7.48</td>\n",
       "      <td>-7.67</td>\n",
       "      <td>-8.17</td>\n",
       "      <td>-8.70</td>\n",
       "      <td>-8.73</td>\n",
       "      <td>-8.73</td>\n",
       "      <td>Russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>114.5</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NETNMIN</td>\n",
       "      <td>2</td>\n",
       "      <td>3000</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>59.64</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-2.39</td>\n",
       "      <td>-7.45</td>\n",
       "      <td>-8.25</td>\n",
       "      <td>-8.49</td>\n",
       "      <td>-8.71</td>\n",
       "      <td>-8.73</td>\n",
       "      <td>-8.73</td>\n",
       "      <td>Russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>114.5</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NETNMIN</td>\n",
       "      <td>1</td>\n",
       "      <td>3000</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>59.64</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.73</td>\n",
       "      <td>-8.71</td>\n",
       "      <td>-8.58</td>\n",
       "      <td>-8.36</td>\n",
       "      <td>-8.33</td>\n",
       "      <td>-8.56</td>\n",
       "      <td>-8.72</td>\n",
       "      <td>-8.73</td>\n",
       "      <td>-8.73</td>\n",
       "      <td>Russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>114.5</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NETNMIN</td>\n",
       "      <td>2</td>\n",
       "      <td>3000</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>59.64</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.73</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-8.71</td>\n",
       "      <td>-8.69</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-8.73</td>\n",
       "      <td>-8.73</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>114.5</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NETNMIN</td>\n",
       "      <td>1</td>\n",
       "      <td>3000</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>59.64</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-8.63</td>\n",
       "      <td>-8.42</td>\n",
       "      <td>-8.48</td>\n",
       "      <td>-8.58</td>\n",
       "      <td>-8.72</td>\n",
       "      <td>-8.73</td>\n",
       "      <td>-8.73</td>\n",
       "      <td>Russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627090</th>\n",
       "      <td>117.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>NETNMIN</td>\n",
       "      <td>2</td>\n",
       "      <td>3000</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>58.26</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.73</td>\n",
       "      <td>-8.70</td>\n",
       "      <td>-8.44</td>\n",
       "      <td>0.55</td>\n",
       "      <td>-8.29</td>\n",
       "      <td>-8.55</td>\n",
       "      <td>-8.73</td>\n",
       "      <td>-8.73</td>\n",
       "      <td>-8.73</td>\n",
       "      <td>Russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627733</th>\n",
       "      <td>117.0</td>\n",
       "      <td>69.5</td>\n",
       "      <td>NETNMIN</td>\n",
       "      <td>1</td>\n",
       "      <td>3000</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>58.26</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.73</td>\n",
       "      <td>-8.68</td>\n",
       "      <td>-8.20</td>\n",
       "      <td>-7.73</td>\n",
       "      <td>-7.92</td>\n",
       "      <td>-8.39</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-8.73</td>\n",
       "      <td>-8.73</td>\n",
       "      <td>Russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627734</th>\n",
       "      <td>117.0</td>\n",
       "      <td>69.5</td>\n",
       "      <td>NETNMIN</td>\n",
       "      <td>2</td>\n",
       "      <td>3000</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>58.26</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.73</td>\n",
       "      <td>-8.71</td>\n",
       "      <td>-7.70</td>\n",
       "      <td>-8.27</td>\n",
       "      <td>-8.36</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-8.73</td>\n",
       "      <td>-8.73</td>\n",
       "      <td>-8.73</td>\n",
       "      <td>Russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628377</th>\n",
       "      <td>117.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>NETNMIN</td>\n",
       "      <td>1</td>\n",
       "      <td>3000</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>61.06</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.73</td>\n",
       "      <td>-8.68</td>\n",
       "      <td>-8.24</td>\n",
       "      <td>-7.80</td>\n",
       "      <td>-7.98</td>\n",
       "      <td>-8.41</td>\n",
       "      <td>-8.72</td>\n",
       "      <td>-8.73</td>\n",
       "      <td>-8.73</td>\n",
       "      <td>Russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628378</th>\n",
       "      <td>117.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>NETNMIN</td>\n",
       "      <td>2</td>\n",
       "      <td>3000</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>61.06</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.73</td>\n",
       "      <td>-8.71</td>\n",
       "      <td>-8.49</td>\n",
       "      <td>-8.27</td>\n",
       "      <td>-8.36</td>\n",
       "      <td>-8.58</td>\n",
       "      <td>-8.73</td>\n",
       "      <td>-8.73</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Russia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4167 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          lon   lat   variable  cohort_number  stand_age  potential_veg  \\\n",
       "2       114.5  66.0   NETNMIN               1       3000              6   \n",
       "3       114.5  66.0   NETNMIN               2       3000              6   \n",
       "10      114.5  66.0   NETNMIN               1       3000              6   \n",
       "11      114.5  66.0   NETNMIN               2       3000              6   \n",
       "14      114.5  66.0   NETNMIN               1       3000              6   \n",
       "...       ...   ...        ...            ...        ...            ...   \n",
       "627090  117.0  69.0   NETNMIN               2       3000              6   \n",
       "627733  117.0  69.5   NETNMIN               1       3000              6   \n",
       "627734  117.0  69.5   NETNMIN               2       3000              6   \n",
       "628377  117.0  70.0   NETNMIN               1       3000              6   \n",
       "628378  117.0  70.0   NETNMIN               2       3000              6   \n",
       "\n",
       "        current_veg  community_type  subtype  silt_clay  ...   Apr   May  \\\n",
       "2                 6               4        4      59.64  ... -8.73 -8.65   \n",
       "3                 6               3        3      59.64  ...  0.00  0.05   \n",
       "10                6               4        4      59.64  ... -8.73 -8.71   \n",
       "11                6               3        3      59.64  ... -8.73  0.00   \n",
       "14                6               4        4      59.64  ...  0.00  0.01   \n",
       "...             ...             ...      ...        ...  ...   ...   ...   \n",
       "627090            6               3        3      58.26  ... -8.73 -8.70   \n",
       "627733            6               4        4      58.26  ... -8.73 -8.68   \n",
       "627734            6               3        3      58.26  ... -8.73 -8.71   \n",
       "628377            6               4        4      61.06  ... -8.73 -8.68   \n",
       "628378            6               3        3      61.06  ... -8.73 -8.71   \n",
       "\n",
       "         Jun   Jul   Aug   Sep   Oct   Nov   Dec   region  \n",
       "2       0.62 -7.48 -7.67 -8.17 -8.70 -8.73 -8.73   Russia  \n",
       "3      -2.39 -7.45 -8.25 -8.49 -8.71 -8.73 -8.73   Russia  \n",
       "10     -8.58 -8.36 -8.33 -8.56 -8.72 -8.73 -8.73   Russia  \n",
       "11     -8.71 -8.69  0.04  0.02 -8.73 -8.73  0.00   Russia  \n",
       "14     -8.63 -8.42 -8.48 -8.58 -8.72 -8.73 -8.73   Russia  \n",
       "...      ...   ...   ...   ...   ...   ...   ...      ...  \n",
       "627090 -8.44  0.55 -8.29 -8.55 -8.73 -8.73 -8.73   Russia  \n",
       "627733 -8.20 -7.73 -7.92 -8.39  0.01 -8.73 -8.73   Russia  \n",
       "627734 -7.70 -8.27 -8.36  0.16 -8.73 -8.73 -8.73   Russia  \n",
       "628377 -8.24 -7.80 -7.98 -8.41 -8.72 -8.73 -8.73   Russia  \n",
       "628378 -8.49 -8.27 -8.36 -8.58 -8.73 -8.73  0.00   Russia  \n",
       "\n",
       "[4167 rows x 31 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()\n",
    "\n",
    "data.loc[data['monthly_mean']>1e10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5351f37b-38e5-41f7-bdbd-1d8695f2c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### save after region\n",
    "all_region_stand_age.to_csv(output_dir_path+region+'/lat_lon_year_standage_var_hist.csv', index=False,float_format='%.2f')\n",
    "all_region.to_csv(output_dir_path+region+'/lat_lon_year_var_hist.csv', index=False,float_format='%.2f')\n",
    "all_region['run_region'] = region\n",
    "\n",
    "all_region_area = all_region.groupby(\n",
    "    ['lon','lat','variable','forest_type','current_veg','community_type','silt_clay','region','run_region']\n",
    ")[all_region.columns[all_region.columns.str.contains('weight|area')]\n",
    "].sum()\n",
    "\n",
    "all_region_area['monthly_mean'] = (all_region_area['monthly_mean_weight']/ all_region_area['cohort_area'])\n",
    "all_region_area['Jan'] = (all_region_area['Jan_weight']/ all_region_area['cohort_area'])\n",
    "all_region_area['Feb'] = (all_region_area['Feb_weight']/ all_region_area['cohort_area'])\n",
    "all_region_area['Mar'] = (all_region_area['Mar_weight']/ all_region_area['cohort_area'])\n",
    "all_region_area['Apr'] = (all_region_area['Apr_weight']/ all_region_area['cohort_area'])\n",
    "all_region_area['May'] = (all_region_area['May_weight']/ all_region_area['cohort_area'])\n",
    "all_region_area['Jun'] = (all_region_area['Jun_weight']/ all_region_area['cohort_area'])\n",
    "all_region_area['Jul'] = (all_region_area['Jul_weight']/ all_region_area['cohort_area'])\n",
    "all_region_area['Aug'] = (all_region_area['Aug_weight']/ all_region_area['cohort_area'])\n",
    "all_region_area['Sep'] = (all_region_area['Sep_weight']/ all_region_area['cohort_area'])\n",
    "all_region_area['Oct'] = (all_region_area['Oct_weight']/ all_region_area['cohort_area'])\n",
    "all_region_area['Nov'] = (all_region_area['Nov_weight']/ all_region_area['cohort_area'])\n",
    "all_region_area['Dec'] = (all_region_area['Dec_weight']/ all_region_area['cohort_area'])\n",
    "\n",
    "##summarize by lat, lon, var\n",
    "all_region_area = all_region_area.reset_index()\n",
    "\n",
    "all_region_area.to_csv(output_dir_path+region+'/lat_lon_hist.csv', index=False,float_format='%.2f')\n",
    "\n",
    "\n",
    "all_region_year = all_region.groupby(\n",
    "    ['year','variable','forest_type','current_veg','community_type', 'run_region']\n",
    ")[all_region.columns[all_region.columns.str.contains('weight|area')]\n",
    "].sum()\n",
    "\n",
    "all_region_year['monthly_mean'] = (all_region_year['monthly_mean_weight']/ all_region_year['cohort_area'])\n",
    "all_region_year['Jan'] = (all_region_year['Jan_weight']/ all_region_year['cohort_area'])\n",
    "all_region_year['Feb'] = (all_region_year['Feb_weight']/ all_region_year['cohort_area'])\n",
    "all_region_year['Mar'] = (all_region_year['Mar_weight']/ all_region_year['cohort_area'])\n",
    "all_region_year['Apr'] = (all_region_year['Apr_weight']/ all_region_year['cohort_area'])\n",
    "all_region_year['May'] = (all_region_year['May_weight']/ all_region_year['cohort_area'])\n",
    "all_region_year['Jun'] = (all_region_year['Jun_weight']/ all_region_year['cohort_area'])\n",
    "all_region_year['Jul'] = (all_region_year['Jul_weight']/ all_region_year['cohort_area'])\n",
    "all_region_year['Aug'] = (all_region_year['Aug_weight']/ all_region_year['cohort_area'])\n",
    "all_region_year['Sep'] = (all_region_year['Sep_weight']/ all_region_year['cohort_area'])\n",
    "all_region_year['Oct'] = (all_region_year['Oct_weight']/ all_region_year['cohort_area'])\n",
    "all_region_year['Nov'] = (all_region_year['Nov_weight']/ all_region_year['cohort_area'])\n",
    "all_region_year['Dec'] = (all_region_year['Dec_weight']/ all_region_year['cohort_area'])\n",
    "\n",
    "##summarize by lat, lon, var\n",
    "all_region_year = all_region_year.reset_index()\n",
    "\n",
    "all_region_year.to_csv(output_dir_path+region+'/year_hist.csv', index=False,float_format='%.2f')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condaforge",
   "language": "python",
   "name": "condaforge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
